[{"title":"Hello World","url":"/2026/01/12/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new &quot;My New Post&quot;\nMore info: Writing\nRun server$ hexo server\nMore info: Server\nGenerate static files$ hexo generate\nMore info: Generating\nDeploy to remote sites$ hexo deploy\nMore info: Deployment\n"},{"title":"模型融合综述","url":"/2026/01/14/%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88%E7%BB%BC%E8%BF%B0/","content":"原文链接 也可以在Github上看到原文作者的调研成果。\n模型合并方法分类：Pre-merging and During-merging\nIntroductionPre-merging:一般是增加一些prior knowledge\n\nUsing linearized fine-tuning to achieve weight space and input space disentanglement\nPerforming architectural transformations to convert heterogeneous models into homogeneous models\nAligning weights to place them in the same basin\n\nDuring-merging:Basically focus on designing sophisticated techniques to merge multiple models into one.It can be further divided into:\n\nBasic merging methods that perform the simplest parameter merging strategy\nWeighted merging methods that merge multiple models according to the importance calculated by specific rules\nSubspace merging methods that project multiple models into sparse subspaces for merging\nRouting-based methods that dynamically merge models according to input samples during inference\nThe post-calibration based method that corrects the merged model\n\nModel Merging MethodsNotation and Probelm definitonAssume there are  $T$ models $\\Phi{\\Theta^{(1)}}, \\dots, \\Phi{\\Theta^{(T)}}$ of the same architecture that need to be merged, and they train from scratch or fine-tune on the same pre-trained model $\\Phi{\\Theta^{(0)}}$  respectively. The parameters (or weights) of the $t$ -th model  $\\Phi{\\Theta^{(t)}}$ are represented as  $\\Theta^{(t)} = {\\Thetal^{(t)}}{l=1}^L$, where $l$ denotes the $l$-th layer of the model, and $L$ is the total number of layers.\nIn this survey, we focus on parameter-wise merging. In other words, the goal of model merging is to merge the parameters ${\\Theta^{(1)}, \\dots, \\Theta^{(T)}}$, and finally obtain the new parameters $\\Theta^{(merge)} = \\text{merge}(\\Theta^{(1)}, \\dots, \\Theta^{(T)})$. One straightforward solution for merging models is weighted averaging, defined as $\\Theta^{(merge)} = \\frac{1}{T} \\sum_{t=1}^T \\Theta^{(t)}$. However, the performance of this approach is often unacceptably poor or infeasible due to several possible factors: (i) The lack of suitable merging conditions, such as multiple models not being in the same basin or having inconsistent architectures. (ii) There are conflicts and interference among multiple models. We illustrate how advanced methods address these issues later, respectively.\nLinearization Fine-tuningarXiv 2305.12827 Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models解释了任务算术效率较高的原因：在切空间进行操作有利于权重解耦。\narXiv 2212.04089 Editing Models with Task Arthimetic  提出基于任务向量进行模型合并\n","tags":["论文分享"]},{"title":"25-26数据可视化课程报告","url":"/2026/01/13/25-26%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E8%AF%BE%E7%A8%8B%E6%8A%A5%E5%91%8A/","content":"\n下载完整报告 PDF\n","tags":["课程报告"]}]